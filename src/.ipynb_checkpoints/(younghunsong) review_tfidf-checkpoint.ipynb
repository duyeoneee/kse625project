{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2051\n",
      "{'city': {'Toronto'}, 'categories': {'Seafood', 'Japanese', 'Chinese', 'Italian', 'Burgers'}}\n",
      "['business_id']\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import JSONLoader\n",
    "fields = ['business_id']\n",
    "city = ['Toronto']\n",
    "categories = ['Burgers','Seafood','Italian','Chinese','Japanese']\n",
    "\n",
    "business = 'business.json'\n",
    "review = 'review.json'\n",
    "data_dir = 'data/dataset'\n",
    "\n",
    "jl = JSONLoader(business, data_dir, fields = fields, encoding = 'utf-8')\n",
    "jl.set_condition(city=city, categories=categories)\n",
    "f_b, business_id = jl.sample(10000000)\n",
    "business_id = set([i[0] for i in business_id])\n",
    "print(len(business_id))\n",
    "print(jl.condition)\n",
    "print(jl.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96936\n"
     ]
    }
   ],
   "source": [
    "fields = ['business_id','text']\n",
    "jl = JSONLoader(review, data_dir, fields = fields)\n",
    "jl.set_condition(business_id = business_id)\n",
    "f_, rv = jl.sample(10000000)\n",
    "print(len(rv)) # The total number of review texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "# remove common words and tokenize\n",
    "documents = [r[-1] for r in rv]\n",
    "texts = [[word for word in document.lower().split() if word not in stops]\n",
    "         for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 5] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(38627 unique tokens: ['smashed', 'burgers', 'done', 'properly', 'heart']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "import pickle\n",
    "with open('./data/tfidf_dictionary', 'wb') as f:\n",
    "    pickle.dump(dictionary, f)\n",
    "    \n",
    "from gensim import corpora, models, similarities\n",
    "import pickle\n",
    "with open('./data/tfidf_dictionary', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "with open('./data/tfidf_model', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14, 0.1659690338338441),\n",
       " (65, 0.08172837263365922),\n",
       " (70, 0.04824371869141254),\n",
       " (78, 0.03512631239538303),\n",
       " (160, 0.13690132609190717),\n",
       " (183, 0.13225873142384892),\n",
       " (199, 0.06682204117194206),\n",
       " (212, 0.055173113407135835),\n",
       " (237, 0.1503173389266708),\n",
       " (339, 0.16043657281149443),\n",
       " (340, 0.16239727437512821),\n",
       " (352, 0.15166070532564258),\n",
       " (372, 0.11676911834871256),\n",
       " (406, 0.09922037988337243),\n",
       " (490, 0.1470240025479285),\n",
       " (505, 0.19235607503939242),\n",
       " (728, 0.0978379601618346),\n",
       " (768, 0.0966187101253497),\n",
       " (805, 0.09890238801833307),\n",
       " (827, 0.07452444317271396),\n",
       " (830, 0.12241572481030523),\n",
       " (843, 0.11111357457224348),\n",
       " (851, 0.11659097105824347),\n",
       " (852, 0.18718103515580228),\n",
       " (922, 0.1690244502630934),\n",
       " (1015, 0.11831252053158219),\n",
       " (1081, 0.15220678330509943),\n",
       " (1247, 0.12581503630716068),\n",
       " (1323, 0.1168079520720305),\n",
       " (1356, 0.19455387001170182),\n",
       " (1487, 0.1573194100126931),\n",
       " (1492, 0.16649951677839323),\n",
       " (1539, 0.16518951251215008),\n",
       " (1549, 0.14092805828084956),\n",
       " (1573, 0.1401508819991814),\n",
       " (2806, 0.19082170175083538),\n",
       " (2836, 0.14963733077876937),\n",
       " (3071, 0.2316111272862066),\n",
       " (3137, 0.1596740585827468),\n",
       " (4834, 0.19057164308670252),\n",
       " (5209, 0.19670560785619765),\n",
       " (7681, 0.22599778101408335),\n",
       " (20926, 0.33482667831752766)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('./data/tfidf_model', 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "tfidf[corpus[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (11, 0.0), (12, 0.0), (13, 0.0), (14, 0.0), (15, 0.0), (16, 0.0), (17, 0.040174279), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.021136817), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (36, 0.0), (37, 0.0), (38, 0.0), (39, 0.0), (40, 0.0), (41, 0.0), (42, 0.023891667), (43, 0.0), (44, 0.0), (45, 0.037597351), (46, 0.15014948), (47, 0.0), (48, 0.0), (49, 0.0), (50, 0.0), (51, 0.0), (52, 0.0), (53, 0.0), (54, 0.048511535), (55, 0.0), (56, 0.0), (57, 0.0), (58, 0.023922624), (59, 0.034806591), (60, 0.0), (61, 0.0), (62, 0.13383804), (63, 0.0), (64, 0.0), (65, 0.0), (66, 0.016036035), (67, 0.0), (68, 0.02094641), (69, 0.015173888), (70, 0.0), (71, 0.0), (72, 0.0), (73, 0.0), (74, 0.0), (75, 0.038370732), (76, 0.0), (77, 0.0), (78, 0.0), (79, 0.15158795), (80, 0.014049986), (81, 0.0), (82, 0.0), (83, 0.0), (84, 0.0), (85, 0.0), (86, 0.0), (87, 0.0), (88, 0.0), (89, 0.0), (90, 0.0), (91, 0.0), (92, 0.0), (93, 0.062345177), (94, 0.0), (95, 0.0), (96, 0.0), (97, 0.0), (98, 0.0), (99, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = 'this chinese restaurant is awesome'\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus[:100]], len(dictionary))\n",
    "sims = index[tfidf[new_vec]] # similarity between new_vec and all other corpus\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TF-IDF vectors for a specific category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes quite long\n",
    "\n",
    "categories =['Burgers','Seafood','Italian','Chinese','Japanese']\n",
    "data_dir = 'data/dataset'\n",
    "with open('./data/tfidf_dictionary', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "with open('./data/tfidf_model', 'rb') as f:\n",
    "    tfidf = pickle.load(f)    \n",
    "\n",
    "def get_category_doc(category, data_dir, dictionary):\n",
    "    from utils.preprocess import JSONLoader\n",
    "    fields = ['business_id']\n",
    "    city = ['Toronto']\n",
    "    categories = [category]\n",
    "    business = 'business.json'\n",
    "    review = 'review.json'\n",
    "\n",
    "    jl = JSONLoader(business, data_dir, fields = fields, encoding = 'utf-8')\n",
    "    jl.set_condition(city=city, categories=categories)\n",
    "    f_b, business_id = jl.sample(10000000)\n",
    "    business_id = set([i[0] for i in business_id])\n",
    "    fields = ['business_id','text']\n",
    "    jl = JSONLoader(review, data_dir, fields = fields)\n",
    "    jl.set_condition(business_id = business_id)\n",
    "    f_, rv = jl.sample(10000000)\n",
    "    \n",
    "    from itertools import chain\n",
    "    rv_flat = list(chain(*[r[-1].lower().split() for r in rv]))\n",
    "    doc = dictionary.doc2bow(rv_flat)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "doc_cat = [get_category_doc(cat, data_dir, dictionary) for cat in categories]\n",
    "\n",
    "import pickle\n",
    "with open('./data/category_doc', 'wb') as f:\n",
    "    pickle.dump(doc_cat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burgers 0.0305375\n",
      "Seafood 0.0575153\n",
      "Italian 0.0448352\n",
      "Chinese 0.145104\n",
      "Japanese 0.0421291\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('./data/category_doc', 'rb') as f:\n",
    "    doc_cat = pickle.load(f)\n",
    "new_doc = 'this chinese restaurant is awesome'\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "\n",
    "def cosine_sim(v, doc_cat, tfidf, dictionary):\n",
    "    from gensim import similarities\n",
    "    index = similarities.SparseMatrixSimilarity(tfidf[doc_cat], len(dictionary))\n",
    "    sims = index[tfidf[v]] # similarity between v and all other corpus\n",
    "    \n",
    "    return sims\n",
    "\n",
    "categories = ['Burgers','Seafood','Italian','Chinese','Japanese']\n",
    "for i, s in enumerate(cosine_sim(new_vec, doc_cat, tfidf, dictionary)):\n",
    "    print(categories[i], s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TF-IDF vectors for each businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_business_doc(data_dir, dictionary):\n",
    "    '''\n",
    "    Returns:\n",
    "        business_doc (dict): Dictionary of (business_id, tf-idf vector).\n",
    "        The tf-idf vector is calculated for the reviews for the businesses in Toronto,\n",
    "        and whose categories are one of the 'Burgers','Seafood','Italian','Chinese','Japanese'.\n",
    "    '''\n",
    "    from utils.preprocess import JSONLoader\n",
    "    fields = ['business_id']\n",
    "    city = ['Toronto']\n",
    "    categories = ['Burgers','Seafood','Italian','Chinese','Japanese']\n",
    "    business = 'business.json'\n",
    "    review = 'review.json'\n",
    "\n",
    "    jl = JSONLoader(business, data_dir, fields = fields, encoding = 'utf-8')\n",
    "    jl.set_condition(city=city, categories=categories)\n",
    "    f_b, business_id = jl.sample(10000000)\n",
    "    business_id = set([i[0] for i in business_id])\n",
    "    \n",
    "    # Get the reviews for the businesses in Toronto, and whose\n",
    "    # categories are 'Burgers','Seafood','Italian','Chinese','Japanese'\n",
    "    fields = ['business_id','text']\n",
    "    jl = JSONLoader(review, data_dir, fields = fields)\n",
    "    jl.set_condition(business_id = business_id)\n",
    "    f_, rv = jl.sample(10000000)\n",
    "    \n",
    "    # dicionary of business, docs pair\n",
    "    from collections import defaultdict\n",
    "    business_doc = defaultdict(lambda: [])\n",
    "    for b_id, b_rv in rv:\n",
    "        doc = dictionary.doc2bow(b_rv.lower().split())\n",
    "        business_doc[b_id].extend(doc)\n",
    "    \n",
    "    return business_doc\n",
    "\n",
    "doc_business = get_business_doc(data_dir, dictionary)\n",
    "doc_business = dict(doc_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/business_doc', 'wb') as f:\n",
    "    pickle.dump(doc_business, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/business_doc', 'rb') as f:\n",
    "    doc_business = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.77067816,  0.44591644,  0.41924542,  0.38793871,  0.3955414 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(doc_business[list(doc_business.keys())[0]], doc_cat, tfidf, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the tf-idf between the categories and businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burgers 0.770678\n",
      "Seafood 0.445916\n",
      "Italian 0.419245\n",
      "Chinese 0.387939\n",
      "Japanese 0.395541\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "categories =['Burgers','Seafood','Italian','Chinese','Japanese']\n",
    "data_dir = 'data/dataset'\n",
    "with open('./data/tfidf_dictionary', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "with open('./data/tfidf_model', 'rb') as f:\n",
    "    tfidf = pickle.load(f)    \n",
    "with open('./data/category_doc', 'rb') as f:\n",
    "    doc_cat = pickle.load(f)\n",
    "with open('./data/business_doc', 'rb') as f:\n",
    "    doc_business = pickle.load(f)\n",
    "\n",
    "def cosine_sim(v, doc_cat, tfidf, dictionary):\n",
    "    from gensim import similarities\n",
    "    index = similarities.SparseMatrixSimilarity(tfidf[doc_cat], len(dictionary))\n",
    "    sims = index[tfidf[v]] # similarity between v and all other corpus\n",
    "    \n",
    "    return sims\n",
    "\n",
    "b_id_test = list(doc_business.keys())[0]\n",
    "\n",
    "for i, s in enumerate(cosine_sim(doc_business[b_id_test], doc_cat, tfidf, dictionary)):\n",
    "    print(categories[i], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77067816"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(doc_business[b_id_test], [doc_cat[0]], tfidf, dictionary)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sim_by_business(doc_business, cat):\n",
    "    column = categories.index(cat)\n",
    "    sim = dict()\n",
    "    for b_id in doc_business:\n",
    "        sim[b_id] = cosine_sim(doc_business[b_id], [doc_cat[column]], tfidf, dictionary)[0]\n",
    "    \n",
    "    return sorted(sim.items(), key=lambda x: -x[1])\n",
    "\n",
    "import pickle\n",
    "sims = dict()\n",
    "for cat in categories:\n",
    "    sims = sim_by_business(doc_business, cat)\n",
    "    with open('./data/cos_sim_%s' % cat, 'wb') as f:\n",
    "        pickle.dump(sims, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the final tf-idf similarity between the businesses / categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "categories =['Burgers','Seafood','Italian','Chinese','Japanese']\n",
    "sims = dict()\n",
    "for cat in categories:\n",
    "    with open('./data/cos_sim_%s' % cat, 'rb') as f:\n",
    "        sims[cat] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zgQHtqX0gqMw1nlBZl2VnQ', 3.5928981),\n",
       " ('RtUvSWO_UZ8V3Wpj0n077w', 3.4973285),\n",
       " ('O1TvPrgkK2bUo5O5aSZ7lw', 3.467016),\n",
       " ('f5O7v_X_jCg2itqacRfxhg', 2.9844866),\n",
       " ('fGurvC5BdOfd5MIuLUQYVA', 2.9722822),\n",
       " ('BUcTdN-rNE8urCCQuxSOQA', 2.9080408),\n",
       " ('DE89UdHFMCN6DtYWZuer5A', 2.8275206),\n",
       " ('_xAJZOKBMPOe47p1MphB2w', 2.7320778),\n",
       " ('RwRNR4z3kY-4OsFqigY5sw', 2.4913545),\n",
       " ('OllK5_S-7svgSwbUfx1xYA', 2.4909453)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims['Chinese'][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
